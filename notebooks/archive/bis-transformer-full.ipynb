{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12508715,"sourceType":"datasetVersion","datasetId":7895119}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Cell 1: Setup & Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport glob\n\nprint(\"--- Libraries Imported ---v2\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-23T09:35:20.800931Z","iopub.execute_input":"2025-07-23T09:35:20.801130Z","iopub.status.idle":"2025-07-23T09:35:35.841026Z","shell.execute_reply.started":"2025-07-23T09:35:20.801104Z","shell.execute_reply":"2025-07-23T09:35:35.840222Z"}},"outputs":[{"name":"stderr","text":"2025-07-23 09:35:24.208303: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753263324.376691      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753263324.427765      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"--- Libraries Imported ---v2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Cell 2: Configuration","metadata":{}},{"cell_type":"code","source":"# config\n\n# input path\nINPUT_DATA_DIR = '/kaggle/input/vitaldb-preprocessed-data' \nMASTER_DATASET_PATH = os.path.join(INPUT_DATA_DIR, 'vitaldb_master_dataset.parquet')\n\n# output path\nOUTPUT_DIR = '/kaggle/working/'\nSCALER_PATH = os.path.join(OUTPUT_DIR, 'final_scaler.joblib')\nX_TRAIN_PATH = os.path.join(OUTPUT_DIR, 'X_train.npy')\nY_TRAIN_PATH = os.path.join(OUTPUT_DIR, 'y_train.npy')\nX_VAL_PATH = os.path.join(OUTPUT_DIR, 'X_val.npy')\nY_VAL_PATH = os.path.join(OUTPUT_DIR, 'y_val.npy')\nMODEL_SAVE_PATH = os.path.join(OUTPUT_DIR, 'small_transformer_best_model.keras')\n\n# training parameters\nSEQUENCE_LENGTH = 300\nBATCH_SIZE = 512\nEPOCHS = 5\nLEARNING_RATE = 1e-4\nVALIDATION_SPLIT_SIZE = 0.10    # 90% data for training and 10% for testing  \nRANDOM_SEED = 42\n\nprint(\"--- Configuration Set ---v2\")\nprint(f\"Scaler will be saved to: {SCALER_PATH}\")\nprint(f\"Sequences will be saved to: {OUTPUT_DIR}\")\nprint(f\"Best model will be saved to: {MODEL_SAVE_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T09:35:35.842428Z","iopub.execute_input":"2025-07-23T09:35:35.842875Z","iopub.status.idle":"2025-07-23T09:35:35.849263Z","shell.execute_reply.started":"2025-07-23T09:35:35.842856Z","shell.execute_reply":"2025-07-23T09:35:35.848601Z"}},"outputs":[{"name":"stdout","text":"--- Configuration Set ---v2\nScaler will be saved to: /kaggle/working/final_scaler.joblib\nSequences will be saved to: /kaggle/working/\nBest model will be saved to: /kaggle/working/small_transformer_best_model.keras\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Cell 3: Data Loading & Patient-Aware Splitting\n","metadata":{}},{"cell_type":"code","source":"print(\"--- Loading Master Dataset ---v2\")\nmaster_df = pd.read_parquet(MASTER_DATASET_PATH)\n\nprint(f\"Loaded {len(master_df)} rows from {master_df['patient_id'].nunique()} patients.\")\n\nprint(\"--- Performing Patient-Aware Split ---\")\nall_patient_ids = master_df['patient_id'].unique()\n\ntrain_ids, val_ids = train_test_split(\n    all_patient_ids,\n    test_size=VALIDATION_SPLIT_SIZE,\n    random_state=RANDOM_SEED\n)\n\ntrain_df = master_df[master_df['patient_id'].isin(train_ids)]\nval_df = master_df[master_df['patient_id'].isin(val_ids)]\n\nprint(f\"Training data: {len(train_df)} rows from {len(train_ids)} patients.\")\nprint(f\"Validation data: {len(val_df)} rows from {len(val_ids)} patients.\")\n\ndel master_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T09:35:35.850127Z","iopub.execute_input":"2025-07-23T09:35:35.850422Z","iopub.status.idle":"2025-07-23T09:35:42.425019Z","shell.execute_reply.started":"2025-07-23T09:35:35.850397Z","shell.execute_reply":"2025-07-23T09:35:42.424388Z"}},"outputs":[{"name":"stdout","text":"--- Loading Master Dataset ---v2\nLoaded 44988566 rows from 3237 patients.\n--- Performing Patient-Aware Split ---\nTraining data: 40635720 rows from 2913 patients.\nValidation data: 4352846 rows from 324 patients.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Cell 4: Scaler Fitting (with Persistence)\n","metadata":{}},{"cell_type":"code","source":"feature_cols = ['Solar8000/HR', 'Solar8000/ART_MBP', 'Solar8000/PLETH_SPO2', 'Solar8000/ETCO2']\n\nif os.path.exists(SCALER_PATH):\n    print(f\"--- Loading existing scaler from {SCALER_PATH} ---\")\n    scaler = joblib.load(SCALER_PATH)\nelse:\n    print(\"--- Fitting new scaler ON TRAINING DATA ONLY ---v2\")\n    scaler = StandardScaler()\n    scaler.fit(train_df[feature_cols])\n    print(f\"--- Saving scaler to {SCALER_PATH} ---\")\n    joblib.dump(scaler, SCALER_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T09:35:42.425601Z","iopub.execute_input":"2025-07-23T09:35:42.425781Z","iopub.status.idle":"2025-07-23T09:35:43.936333Z","shell.execute_reply.started":"2025-07-23T09:35:42.425766Z","shell.execute_reply":"2025-07-23T09:35:43.935562Z"}},"outputs":[{"name":"stdout","text":"--- Fitting new scaler ON TRAINING DATA ONLY ---v2\n--- Saving scaler to /kaggle/working/final_scaler.joblib ---\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Cell 5: Sequence Generation\n","metadata":{}},{"cell_type":"code","source":"# --- Cell 5: High-Performance Ragged Tensor Pipeline (Pure Graph Windowing) ---\n\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nimport numpy as np\n\ndef calculate_steps_per_epoch(df, sequence_length, batch_size):\n    \"\"\"Calculates the number of steps per epoch.\"\"\"\n    total_sequences = 0\n    grouped = df.groupby('patient_id')\n    for _, group in tqdm(grouped, desc=\"Calculating total sequences\", total=df['patient_id'].nunique()):\n        if len(group) >= sequence_length:\n            total_sequences += len(group) - sequence_length + 1\n    steps = int(np.ceil(total_sequences / batch_size))\n    return steps\n\ndef create_ragged_tensor_dataset(df, scaler, feature_cols, target_col, sequence_length, batch_size, shuffle=False):\n    \"\"\"\n    The definitive high-performance pipeline.\n    1. Manually constructs Ragged Tensors to avoid slow loading.\n    2. Uses tf.signal.frame for pure, in-graph windowing, avoiding all sub-dataset overhead.\n    \"\"\"\n    print(f\"\\n--- Building DEFINITIVE Ragged Tensor pipeline (shuffle={shuffle}) ---\")\n    df_scaled = df.copy()\n    df_scaled[feature_cols] = scaler.transform(df[feature_cols])\n\n    grouped = df_scaled.groupby('patient_id')\n    all_patient_features = []\n    all_patient_targets = []\n    for _, group in tqdm(grouped, desc=\"Extracting patient data\", total=df['patient_id'].nunique()):\n        if len(group) < sequence_length:\n            continue\n        all_patient_features.append(group[feature_cols].values)\n        all_patient_targets.append(group[target_col].values)\n\n    print(\"--- Manually constructing Ragged Tensors ---\")\n    feature_values_list = [item for item in all_patient_features]\n    target_values_list = [item for item in all_patient_targets]\n    feature_row_splits = [0] + np.cumsum([len(p) for p in all_patient_features]).tolist()\n    target_row_splits = [0] + np.cumsum([len(p) for p in all_patient_targets]).tolist()\n\n    final_feature_values = np.concatenate(feature_values_list, axis=0).astype(np.float32)\n    final_target_values = np.concatenate(target_values_list, axis=0).astype(np.float32)\n\n    ragged_features = tf.RaggedTensor.from_row_splits(values=final_feature_values, row_splits=feature_row_splits)\n    ragged_targets = tf.RaggedTensor.from_row_splits(values=final_target_values, row_splits=target_row_splits)\n\n    dataset = tf.data.Dataset.from_tensor_slices((ragged_features, ragged_targets))\n\n    # --- THE DEFINITIVE FIX: Pure Tensor-based windowing ---\n    def create_sequences_from_tensors(features, targets):\n        # Use tf.signal.frame to create windows directly from tensors. This is a single, fast C++ op.\n        feature_sequences = tf.signal.frame(features, frame_length=sequence_length, frame_step=1, axis=0)\n\n        # For targets, we need the value at the end of each sequence.\n        # We can get this by windowing the 1D target tensor and taking the last element of each window.\n        target_sequences = tf.signal.frame(targets, frame_length=sequence_length, frame_step=1, axis=0)\n        target_labels = target_sequences[:, -1] # Take the last value of each sequence\n\n        return tf.data.Dataset.from_tensor_slices((feature_sequences, target_labels))\n\n    # flat_map now applies this highly efficient, pure-tensor function to each patient.\n    dataset = dataset.flat_map(create_sequences_from_tensors)\n    # --- END OF DEFINITIVE FIX ---\n\n    if shuffle:\n        dataset = dataset.shuffle(buffer_size=1024*4)\n\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n\n    return dataset\n\nprint(\"--- High-performance Ragged Tensor pipeline functions defined. ---v7 (Pure Graph Windowing)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T09:35:43.938083Z","iopub.execute_input":"2025-07-23T09:35:43.938329Z","iopub.status.idle":"2025-07-23T09:35:43.949076Z","shell.execute_reply.started":"2025-07-23T09:35:43.938311Z","shell.execute_reply":"2025-07-23T09:35:43.948410Z"}},"outputs":[{"name":"stdout","text":"--- High-performance Ragged Tensor pipeline functions defined. ---v7 (Pure Graph Windowing)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Cell 6: Model Definition\n","metadata":{}},{"cell_type":"code","source":"# --- Cell 6: Model Definition & Simplified Callback ---\n\nimport json\nimport shutil\nimport os\n\n# 1. SIMPLIFIED, ROBUST Resumable Checkpoint Callback\nclass ResumableCheckpoint(keras.callbacks.Callback):\n    def __init__(self, filepath, state_filepath, save_freq, log_freq=500):\n        super().__init__()\n        self.filepath = filepath\n        self.state_filepath = state_filepath\n        self.save_freq = save_freq # How many batches between model saves\n        self.log_freq = log_freq   # How many batches between log messages\n        self.current_epoch = 0\n        self.batches_seen_this_epoch = 0\n\n    def on_epoch_begin(self, epoch, logs=None):\n        # Store the current epoch (it's 0-indexed)\n        self.current_epoch = epoch\n        # Reset batch counter at the start of a new epoch\n        self.batches_seen_this_epoch = 0\n\n    def on_batch_end(self, batch, logs=None):\n        # `batch` is the batch index within the current epoch\n        self.batches_seen_this_epoch = batch + 1\n        if self.batches_seen_this_epoch % self.log_freq == 0:\n            loss = logs.get('loss', 'N/A'); mae = logs.get('mean_absolute_error', 'N/A')\n            print(f\"\\n... Heartbeat: Reached Epoch {self.current_epoch + 1}, Batch {self.batches_seen_this_epoch}. Current loss: {loss:.4f}, MAE: {mae:.4f} ...\")\n\n        # Save a temporary model checkpoint periodically as a safeguard\n        if self.batches_seen_this_epoch % self.save_freq == 0:\n            print(f\"\\n--- Safeguard checkpoint: saving model at epoch {self.current_epoch + 1}, batch {self.batches_seen_this_epoch} ---\")\n            # Save to a temporary path to ensure atomicity\n            root, ext = os.path.splitext(self.filepath)\n            temp_filepath = root + '_temp' + ext\n            self.model.save(temp_filepath)\n            # Atomically move the file to the final destination\n            shutil.move(temp_filepath, self.filepath)\n            print(\"--- Safeguard checkpoint saved successfully. ---\")\n\n    def on_epoch_end(self, epoch, logs=None):\n        \"\"\"Saves the state to indicate that the next epoch should begin.\"\"\"\n        print(f\"\\n--- Epoch {epoch + 1} complete. Updating state file for resumption at Epoch {epoch + 2}. ---\")\n        # Save a state that indicates this epoch is done and the next one should start.\n        # We use `epoch + 1` because the `epoch` argument is 0-indexed.\n        state = {\"epoch\": epoch + 1}\n        with open(self.state_filepath, 'w') as f:\n            json.dump(state, f)\n        print(\"--- State file updated for next epoch. ---\")\n\n\n# 2. Transformer Block Definition (Unchanged)\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n        super().__init__(**kwargs)\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential([layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim)])\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6); self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate); self.dropout2 = layers.Dropout(rate)\n    def call(self, inputs, training=False):\n        input_dtype = inputs.dtype; ln_input = self.layernorm1(inputs)\n        attn_output = self.att(ln_input, ln_input)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = inputs + tf.cast(attn_output, dtype=input_dtype)\n        ln_out1 = self.layernorm2(out1); ffn_output = self.ffn(ln_out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return out1 + tf.cast(ffn_output, dtype=input_dtype)\n\n# 3. Model Builder (Unchanged)\ndef build_optimized_transformer_model(sequence_length, num_features):\n    embed_dim=32; num_heads=4; ff_dim=128; num_blocks=2; dropout_rate=0.15\n    print(f\"\\n--- Building OPTIMIZED model: embed_dim={embed_dim}, ff_dim={ff_dim}, num_heads={num_heads} ---\")\n    inputs = keras.Input(shape=(sequence_length, num_features)); x = layers.Dense(embed_dim)(inputs)\n    for _ in range(num_blocks): x = TransformerBlock(embed_dim, num_heads, ff_dim, rate=dropout_rate)(x)\n    x = layers.GlobalAveragePooling1D()(x); x = layers.Dropout(dropout_rate)(x)\n    x = layers.Dense(16, activation=\"gelu\")(x); outputs = layers.Dense(1, dtype='float32')(x)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    print(f\"Model created with ~{model.count_params():,} parameters.\"); return model\n\nprint(\"--- Optimized model architecture and SIMPLIFIED ResumableCheckpoint defined ---v3\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T09:35:43.949943Z","iopub.execute_input":"2025-07-23T09:35:43.950188Z","iopub.status.idle":"2025-07-23T09:35:43.974192Z","shell.execute_reply.started":"2025-07-23T09:35:43.950171Z","shell.execute_reply":"2025-07-23T09:35:43.973520Z"}},"outputs":[{"name":"stdout","text":"--- Optimized model architecture and SIMPLIFIED ResumableCheckpoint defined ---v3\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Cell 7: Training\n","metadata":{}},{"cell_type":"code","source":"# --- Cell 7: High-Performance & Resumable Training (Final Tuning) ---\n\nRESUMABLE_MODEL_PATH = os.path.join(OUTPUT_DIR, 'resumable_checkpoint.keras')\nSTATE_FILE_PATH = os.path.join(OUTPUT_DIR, 'training_state.json')\n\n# --- Step 1: Handle Resumption State ---\ninitial_epoch = 0\nmodel = None # Initialize model as None\n\nif os.path.exists(STATE_FILE_PATH) and os.path.exists(RESUMABLE_MODEL_PATH):\n    print(f\"--- Found existing checkpoint. Resuming training. ---\")\n    with open(STATE_FILE_PATH, 'r') as f:\n        state = json.load(f)\n    initial_epoch = state.get('epoch', 0)\n\n    print(f\"--- Resuming from epoch {initial_epoch + 1}. ---\")\n    model = keras.models.load_model(RESUMABLE_MODEL_PATH, custom_objects={\"TransformerBlock\": TransformerBlock})\nelse:\n    print(\"--- No existing checkpoint found. Starting from scratch. ---\")\n\n\n# --- Step 2: Calculate Steps Per Epoch BEFORE creating datasets ---\n# This is now CRITICAL for performance.\nprint(\"--- Pre-calculating steps per epoch for train and validation sets... ---\")\ntrain_steps = calculate_steps_per_epoch(train_df, SEQUENCE_LENGTH, BATCH_SIZE)\nval_steps = calculate_steps_per_epoch(val_df, SEQUENCE_LENGTH, BATCH_SIZE)\nprint(f\"--- Calculated {train_steps} train steps and {val_steps} validation steps per epoch. ---\")\n\n\n# --- Step 3: Create the Data Pipelines ---\nprint(\"\\n--- Creating high-performance in-graph datasets... ---\")\ntrain_dataset = create_ragged_tensor_dataset(\n    train_df, scaler, feature_cols, 'BIS/BIS', SEQUENCE_LENGTH, BATCH_SIZE, shuffle=True\n)\nval_dataset = create_ragged_tensor_dataset(\n    val_df, scaler, feature_cols, 'BIS/BIS', SEQUENCE_LENGTH, BATCH_SIZE\n)\nprint(\"\\n--- tf.data in-graph pipelines created successfully. ---\")\ndel train_df, val_df\n\n\n# --- Step 4: Build Model if Needed ---\nif model is None:\n    print(\"--- Building new model from scratch. ---\")\n    model = build_optimized_transformer_model(SEQUENCE_LENGTH, len(feature_cols))\n\n\n# --- Step 5: Compile and Train ---\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\n\n# Define LR schedule using the pre-calculated steps\ntotal_steps_for_lr = train_steps * EPOCHS\nwarmup_steps = int(total_steps_for_lr * 0.1)\nprint(f\"--- LR Schedule: Total Steps={total_steps_for_lr}, Warmup Steps={warmup_steps} ---\")\n\nlr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=LEARNING_RATE,\n    decay_steps=total_steps_for_lr - warmup_steps,\n    end_learning_rate=0.0,\n    power=1.0\n)\n\noptimizer = tf.keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=0.01)\nmodel.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_absolute_error'])\nmodel.summary()\n\n# Define Callbacks\nbest_model_checkpoint = keras.callbacks.ModelCheckpoint(\n    MODEL_SAVE_PATH, monitor='val_loss', save_best_only=True, mode='min', verbose=1\n)\nresumable_checkpoint = ResumableCheckpoint(\n    filepath=RESUMABLE_MODEL_PATH, state_filepath=STATE_FILE_PATH, save_freq=2000, log_freq=500\n)\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=2, restore_best_weights=True, verbose=1\n)\n\nprint(f\"\\n--- Starting FULLY OPTIMIZED & RESUMABLE Training ---\")\nhistory = model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=EPOCHS,\n    callbacks=[best_model_checkpoint, resumable_checkpoint, early_stopping],\n    initial_epoch=initial_epoch,\n    steps_per_epoch=train_steps,       # <-- CRITICAL FIX\n    validation_steps=val_steps         # <-- CRITICAL FIX\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T09:35:43.974900Z","iopub.execute_input":"2025-07-23T09:35:43.975161Z"}},"outputs":[{"name":"stdout","text":"--- No existing checkpoint found. Starting from scratch. ---\n--- Pre-calculating steps per epoch for train and validation sets... ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Calculating total sequences:   0%|          | 0/2913 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1aad36b14cdb444ebf4f9471512f5d09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Calculating total sequences:   0%|          | 0/324 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df95d715561a42b5820571959e4ef724"}},"metadata":{}},{"name":"stdout","text":"--- Calculated 77666 train steps and 8313 validation steps per epoch. ---\n\n--- Creating high-performance in-graph datasets... ---\n\n--- Building DEFINITIVE Ragged Tensor pipeline (shuffle=True) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting patient data:   0%|          | 0/2913 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1333e70e06cd4f6ca6a9d259fbd35b8b"}},"metadata":{}},{"name":"stdout","text":"--- Manually constructing Ragged Tensors ---\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1753263354.700141      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"\n--- Building DEFINITIVE Ragged Tensor pipeline (shuffle=False) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting patient data:   0%|          | 0/324 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"482af946f0ee45a4a870dc408692391f"}},"metadata":{}},{"name":"stdout","text":"--- Manually constructing Ragged Tensors ---\n\n--- tf.data in-graph pipelines created successfully. ---\n--- Building new model from scratch. ---\n\n--- Building OPTIMIZED model: embed_dim=32, ff_dim=128, num_heads=4 ---\nModel created with ~51,265 parameters.\n--- LR Schedule: Total Steps=388330, Warmup Steps=38833 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │           \u001b[38;5;34m160\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ transformer_block               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m25,280\u001b[0m │\n│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ transformer_block_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m25,280\u001b[0m │\n│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ transformer_block               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,280</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ transformer_block_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,280</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m51,265\u001b[0m (200.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,265</span> (200.25 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m51,265\u001b[0m (200.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,265</span> (200.25 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n--- Starting FULLY OPTIMIZED & RESUMABLE Training ---\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1753263371.336263     100 service.cc:148] XLA service 0x7bdb6c0069e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1753263371.337095     100 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1753263372.323641     100 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1753263378.046680     100 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  499/77666\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:59:00\u001b[0m 93ms/step - loss: 1167.5730 - mean_absolute_error: 30.0017\n... Heartbeat: Reached Epoch 1, Batch 500. Current loss: 751.9323, MAE: 21.4836 ...\n\u001b[1m  999/77666\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:57:50\u001b[0m 92ms/step - loss: 874.9888 - mean_absolute_error: 23.9243\n... Heartbeat: Reached Epoch 1, Batch 1000. Current loss: 490.4257, MAE: 15.8836 ...\n\u001b[1m 1499/77666\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:56:58\u001b[0m 92ms/step - loss: 731.0272 - mean_absolute_error: 20.8810\n... Heartbeat: Reached Epoch 1, Batch 1500. Current loss: 403.0470, MAE: 13.8758 ...\n\u001b[1m 1999/77666\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:56:15\u001b[0m 92ms/step - loss: 644.3387 - mean_absolute_error: 19.0673\n... Heartbeat: Reached Epoch 1, Batch 2000. Current loss: 364.9089, MAE: 13.2542 ...\n\n--- Safeguard checkpoint: saving model at epoch 1, batch 2000 ---\n--- Safeguard checkpoint saved successfully. ---\n\u001b[1m 2499/77666\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:55:38\u001b[0m 92ms/step - loss: 585.3701 - mean_absolute_error: 17.8457\n... Heartbeat: Reached Epoch 1, Batch 2500. Current loss: 335.9029, MAE: 12.6640 ...\n\u001b[1m 2999/77666\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:54:54\u001b[0m 92ms/step - loss: 541.5890 - mean_absolute_error: 16.9277\n... Heartbeat: Reached Epoch 1, Batch 3000. Current loss: 311.2165, MAE: 12.0687 ...\n\u001b[1m 3499/77666\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:54:05\u001b[0m 92ms/step - loss: 507.2903 - mean_absolute_error: 16.2059\n... Heartbeat: Reached Epoch 1, Batch 3500. Current loss: 296.2756, MAE: 11.7856 ...\n\u001b[1m 3999/77666\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:53:17\u001b[0m 92ms/step - loss: 480.4466 - mean_absolute_error: 15.6415\n... Heartbeat: Reached Epoch 1, Batch 4000. Current loss: 288.0872, MAE: 11.5716 ...\n\n--- Safeguard checkpoint: saving model at epoch 1, batch 4000 ---\n--- Safeguard checkpoint saved successfully. ---\n\u001b[1m 4499/77666\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:52:31\u001b[0m 92ms/step - loss: 458.4913 - mean_absolute_error: 15.1769\n... Heartbeat: Reached Epoch 1, Batch 4500. Current loss: 278.4609, MAE: 11.3637 ...\n\u001b[1m 4999/77666\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:51:45\u001b[0m 92ms/step - loss: 439.9561 - mean_absolute_error: 14.7812\n... Heartbeat: Reached Epoch 1, Batch 5000. Current loss: 269.2754, MAE: 11.1273 ...\n\u001b[1m 5499/77666\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50:59\u001b[0m 92ms/step - loss: 424.2300 - mean_absolute_error: 14.4445\n... Heartbeat: Reached Epoch 1, Batch 5500. Current loss: 264.8415, MAE: 11.0286 ...\n\u001b[1m 5999/77666\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50:12\u001b[0m 92ms/step - loss: 410.6152 - mean_absolute_error: 14.1532\n... Heartbeat: Reached Epoch 1, Batch 6000. Current loss: 258.4236, MAE: 10.8978 ...\n\n--- Safeguard checkpoint: saving model at epoch 1, batch 6000 ---\n--- Safeguard checkpoint saved successfully. ---\n\u001b[1m 6499/77666\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49:26\u001b[0m 92ms/step - loss: 398.6158 - mean_absolute_error: 13.8950\n... Heartbeat: Reached Epoch 1, Batch 6500. Current loss: 251.5913, MAE: 10.7233 ...\n\u001b[1m 6815/77666\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48:56\u001b[0m 92ms/step - loss: 391.7874 - mean_absolute_error: 13.7469","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"Cell 8: Evaluation\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import r2_score\n\nprint(\"\\n--- Evaluating Final Model Performance ---\")\n\n# load best model\nprint(f\"--- Loading best model from {MODEL_SAVE_PATH} ---\")\nloaded_model = keras.models.load_model(\n    MODEL_SAVE_PATH,\n    custom_objects={\"TransformerBlock\": TransformerBlock}\n)\n\n# make predictions on validation set\nprint(\"--- Generating predictions on validation set... ---\")\ny_pred = loaded_model.predict(val_dataset)\n\nprint(\"--- Extracting true labels for comparison... ---\")\ny_true = np.concatenate([y for x, y in val_dataset], axis=0)\n\ny_pred = y_pred[:len(y_true)]\n\nfinal_mae = np.mean(np.abs(y_true - y_pred.squeeze()))\nfinal_mse = np.mean((y_true - y_pred.squeeze())**2)\nfinal_r2 = r2_score(y_true, y_pred)\n\nprint(\"\\n--- Final Model Evaluation on Validation Set ---\")\nprint(f\"  Mean Absolute Error (MAE): {final_mae:.4f}\")\nprint(f\"  Mean Squared Error (MSE):  {final_mse:.4f}\")\nprint(f\"  R-squared (R²):            {final_r2:.4f}\")\nprint(\"-------------------------------------------------\")\n\n# plot training history\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss'); plt.ylabel('Loss (MSE)'); plt.xlabel('Epoch')\nplt.legend(); plt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['mean_absolute_error'], label='Training MAE')\nplt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')\nplt.title('Model MAE'); plt.ylabel('MAE'); plt.xlabel('Epoch')\nplt.legend(); plt.grid(True)\nplt.tight_layout(); plt.show()\n\n# plot predictions vs actual\nplt.figure(figsize=(10, 6))\nsample_size = min(1000, len(y_true))\nplt.scatter(y_true[:sample_size], y_pred[:sample_size], alpha=0.3, label='Predictions')\nplt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], 'r--', lw=2, label='Perfect Fit')\nplt.title('Prediction vs. Actual Values (Sample)'); plt.xlabel('Actual BIS'); plt.ylabel('Predicted BIS')\nplt.legend(); plt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}