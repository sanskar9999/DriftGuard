{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2b2e7e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T04:22:31.751694Z",
     "iopub.status.busy": "2025-07-11T04:22:31.751477Z",
     "iopub.status.idle": "2025-07-11T04:23:01.113548Z",
     "shell.execute_reply": "2025-07-11T04:23:01.112644Z"
    },
    "papermill": {
     "duration": 29.366346,
     "end_time": "2025-07-11T04:23:01.114923",
     "exception": false,
     "start_time": "2025-07-11T04:22:31.748577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vitaldb\r\n",
      "  Downloading vitaldb-1.5.6-py3-none-any.whl.metadata (314 bytes)\r\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\r\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (1.39.1)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vitaldb) (2.32.4)\r\n",
      "Collecting wfdb (from vitaldb)\r\n",
      "  Downloading wfdb-4.3.0-py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\r\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\r\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\r\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\r\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\r\n",
      "Requirement already satisfied: botocore<1.40.0,>=1.39.1 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.39.1)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.0.1)\r\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3) (0.13.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\r\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\r\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.40.0,>=1.39.1->boto3) (2.5.0)\r\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\r\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\r\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vitaldb) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vitaldb) (3.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vitaldb) (2025.6.15)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.11/dist-packages (from wfdb->vitaldb) (3.12.13)\r\n",
      "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb->vitaldb) (2025.5.1)\r\n",
      "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb->vitaldb) (0.13.1)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (1.3.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (1.7.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (6.6.3)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (0.3.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (1.20.1)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.10.0->wfdb->vitaldb) (1.17.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\r\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb->vitaldb) (2.22)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\r\n",
      "Downloading vitaldb-1.5.6-py3-none-any.whl (59 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading wfdb-4.3.0-py3-none-any.whl (163 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: wfdb, vitaldb\r\n",
      "Successfully installed vitaldb-1.5.6 wfdb-4.3.0\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 04:22:42.769030: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752207763.124224      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752207763.231868      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying and Configuring GPUs ---\n",
      "2 GPU(s) detected and configured.\n",
      "-----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Environment Setup ---\n",
    "!pip install vitaldb tensorflow boto3 pandas numpy matplotlib scikit-learn joblib\n",
    "\n",
    "# --- Step 2: Imports ---\n",
    "import os\n",
    "import time\n",
    "import vitaldb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "import joblib # For saving the scaler\n",
    "\n",
    "# --- Step 3: GPU Configuration ---\n",
    "print(\"--- Verifying and Configuring GPUs ---\")\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    try:\n",
    "        for gpu in gpu_devices:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"{len(gpu_devices)} GPU(s) detected and configured.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n",
    "print(\"-----------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e023326d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T04:23:01.121789Z",
     "iopub.status.busy": "2025-07-11T04:23:01.121072Z",
     "iopub.status.idle": "2025-07-11T04:23:01.126773Z",
     "shell.execute_reply": "2025-07-11T04:23:01.126136Z"
    },
    "papermill": {
     "duration": 0.009917,
     "end_time": "2025-07-11T04:23:01.127826",
     "exception": false,
     "start_time": "2025-07-11T04:23:01.117909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONFIGURATION FOR THIS RUN ---\n",
      "Patient Count: 100\n",
      "Epochs: 25\n",
      "Scaler Path: standard_scaler_100.joblib\n",
      "Model Path: vitaldb_bis_transformer_model_100.h5\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration Parameters for the Current Run ---\n",
    "\n",
    "# <<<< SET THE SCALE FOR THIS RUN >>>>\n",
    "# SMOKE TEST: 10 patients\n",
    "# BENCHMARK: 100 patients\n",
    "# FULL RUN: 1000 patients\n",
    "NUM_PATIENTS_TO_PROCESS = 100\n",
    "\n",
    "# <<<< SET EPOCHS FOR THIS RUN >>>>\n",
    "# SMOKE TEST: 1 epoch\n",
    "# BENCHMARK / FULL RUN: 25 epochs\n",
    "EPOCHS = 25\n",
    "\n",
    "# --- Static Parameters ---\n",
    "VITAL_FILES_LOCAL_DIR = f'vital_files_{NUM_PATIENTS_TO_PROCESS}_patients'\n",
    "MODEL_SAVE_PATH = f'vitaldb_bis_transformer_model_{NUM_PATIENTS_TO_PROCESS}.h5'\n",
    "SCALER_SAVE_PATH = f'standard_scaler_{NUM_PATIENTS_TO_PROCESS}.joblib'\n",
    "\n",
    "TRAIN_TEST_SPLIT_RATIO = 0.8\n",
    "RANDOM_SEED = 42\n",
    "SEQUENCE_LENGTH = 300\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 1e-4\n",
    "CLIPNORM = 1.0\n",
    "\n",
    "# Vitals to use as features\n",
    "INPUT_VITALS = ['Solar8000/HR', 'Solar8000/ART_MBP', 'Solar8000/PLETH_SPO2', 'Solar8000/ETCO2']\n",
    "TARGET_VITAL = 'BIS/BIS'\n",
    "\n",
    "print(f\"--- CONFIGURATION FOR THIS RUN ---\")\n",
    "print(f\"Patient Count: {NUM_PATIENTS_TO_PROCESS}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Scaler Path: {SCALER_SAVE_PATH}\")\n",
    "print(f\"Model Path: {MODEL_SAVE_PATH}\")\n",
    "print(f\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dcb1350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T04:23:01.133300Z",
     "iopub.status.busy": "2025-07-11T04:23:01.133097Z",
     "iopub.status.idle": "2025-07-11T04:23:01.148289Z",
     "shell.execute_reply": "2025-07-11T04:23:01.147660Z"
    },
    "papermill": {
     "duration": 0.019172,
     "end_time": "2025-07-11T04:23:01.149285",
     "exception": false,
     "start_time": "2025-07-11T04:23:01.130113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Generator classes defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Transformer Model Definition ---\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.position_embedding = layers.Embedding(input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "    def call(self, inputs):\n",
    "        positions = tf.range(start=0, limit=self.sequence_length, delta=1)\n",
    "        return inputs + self.position_embedding(positions)\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential([layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim)])\n",
    "        self.layernorm1, self.layernorm2 = layers.LayerNormalization(epsilon=1e-6), layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1, self.dropout2 = layers.Dropout(rate), layers.Dropout(rate)\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.dropout1(self.att(inputs, inputs), training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.dropout2(self.ffn(out1), training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# <<<< MODEL PARAMETERS INCREASED HERE >>>>\n",
    "def build_transformer_model(sequence_length, num_features, embed_dim=128, num_heads=8, ff_dim=256, num_blocks=2):\n",
    "    print(f\"\\n--- Building model with increased capacity: embed_dim={embed_dim}, ff_dim={ff_dim}, num_heads={num_heads} ---\")\n",
    "    inputs = keras.Input(shape=(sequence_length, num_features))\n",
    "    x = layers.Dense(embed_dim, activation='relu')(inputs)\n",
    "    x = PositionalEncoding(sequence_length, embed_dim)(x)\n",
    "    for _ in range(num_blocks):\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# --- Memory-Safe, Scaled Data Generator ---\n",
    "class MemorySafeVitalDBGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, name, case_ids, vital_dir, batch_size, sequence_length, feature_cols, target_col, scaler):\n",
    "        self.name = name\n",
    "        self.case_ids = case_ids\n",
    "        self.vital_dir = vital_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_col = target_col\n",
    "        self.scaler = scaler\n",
    "        self.patient_data = []\n",
    "        self.index_map = []\n",
    "        self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        print(f\"\\n--- Preparing {self.name} Data Generator ({len(self.case_ids)} patients) ---\")\n",
    "        total_sequences = 0\n",
    "        for patient_idx, case_id in enumerate(self.case_ids):\n",
    "            file_path = os.path.join(self.vital_dir, f'{case_id:04d}.vital')\n",
    "            if not os.path.exists(file_path): continue\n",
    "            try:\n",
    "                vf = vitaldb.VitalFile(file_path)\n",
    "                df = vf.to_pandas(self.feature_cols + [self.target_col], interval=1)\n",
    "                df.ffill(inplace=True)\n",
    "                df.dropna(inplace=True)\n",
    "                if len(df) > self.sequence_length:\n",
    "                    df[self.feature_cols] = self.scaler.transform(df[self.feature_cols])\n",
    "                    arr = df[self.feature_cols + [self.target_col]].values\n",
    "                    self.patient_data.append(arr)\n",
    "                    num_sequences_in_patient = len(arr) - self.sequence_length\n",
    "                    for seq_idx in range(num_sequences_in_patient):\n",
    "                        self.index_map.append((len(self.patient_data) - 1, seq_idx))\n",
    "                    total_sequences += num_sequences_in_patient\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR: Could not process case {case_id}: {e}\")\n",
    "        print(f\"--- {self.name} Generator ready. Total sequences: {total_sequences} ---\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.index_map) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_index_map = self.index_map[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_x = np.zeros((len(batch_index_map), self.sequence_length, len(self.feature_cols)))\n",
    "        batch_y = np.zeros(len(batch_index_map))\n",
    "        for i, (patient_idx, seq_start_idx) in enumerate(batch_index_map):\n",
    "            seq_end_idx = seq_start_idx + self.sequence_length\n",
    "            data_slice = self.patient_data[patient_idx][seq_start_idx:seq_end_idx]\n",
    "            batch_x[i] = data_slice[:, :-1]\n",
    "            batch_y[i] = data_slice[-1, -1]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "print(\"Model and Generator classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba778740",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T04:23:01.154914Z",
     "iopub.status.busy": "2025-07-11T04:23:01.154712Z",
     "iopub.status.idle": "2025-07-11T04:27:20.237281Z",
     "shell.execute_reply": "2025-07-11T04:27:20.236433Z"
    },
    "papermill": {
     "duration": 259.086983,
     "end_time": "2025-07-11T04:27:20.238677",
     "exception": false,
     "start_time": "2025-07-11T04:23:01.151694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 100 files...\n",
      "--- Download complete. ---\n",
      "\n",
      "Patient Split: 80 for Training, 20 for Validation.\n",
      "\n",
      "--- Fitting new StandardScaler on Training Data ---\n",
      "  ...processing patient 1/80 for scaler...\n",
      "  ...processing patient 2/80 for scaler...\n",
      "  ...processing patient 3/80 for scaler...\n",
      "  ...processing patient 4/80 for scaler...\n",
      "  ...processing patient 5/80 for scaler...\n",
      "  ...processing patient 6/80 for scaler...\n",
      "  ...processing patient 7/80 for scaler...\n",
      "  ...processing patient 8/80 for scaler...\n",
      "  ...processing patient 9/80 for scaler...\n",
      "  ...processing patient 10/80 for scaler...\n",
      "  ...processing patient 11/80 for scaler...\n",
      "  ...processing patient 12/80 for scaler...\n",
      "  ...processing patient 13/80 for scaler...\n",
      "  ...processing patient 14/80 for scaler...\n",
      "  ...processing patient 15/80 for scaler...\n",
      "  ...processing patient 16/80 for scaler...\n",
      "  ...processing patient 17/80 for scaler...\n",
      "  ...processing patient 18/80 for scaler...\n",
      "  ...processing patient 19/80 for scaler...\n",
      "  ...processing patient 20/80 for scaler...\n",
      "  ...processing patient 21/80 for scaler...\n",
      "  ...processing patient 22/80 for scaler...\n",
      "  ...processing patient 23/80 for scaler...\n",
      "  ...processing patient 24/80 for scaler...\n",
      "  ...processing patient 25/80 for scaler...\n",
      "  ...processing patient 26/80 for scaler...\n",
      "  ...processing patient 27/80 for scaler...\n",
      "  ...processing patient 28/80 for scaler...\n",
      "  ...processing patient 29/80 for scaler...\n",
      "  ...processing patient 30/80 for scaler...\n",
      "  ...processing patient 31/80 for scaler...\n",
      "  ...processing patient 32/80 for scaler...\n",
      "  ...processing patient 33/80 for scaler...\n",
      "  ...processing patient 34/80 for scaler...\n",
      "  ...processing patient 35/80 for scaler...\n",
      "  ...processing patient 36/80 for scaler...\n",
      "  ...processing patient 37/80 for scaler...\n",
      "  ...processing patient 38/80 for scaler...\n",
      "  ...processing patient 39/80 for scaler...\n",
      "  ...processing patient 40/80 for scaler...\n",
      "  ...processing patient 41/80 for scaler...\n",
      "  ...processing patient 42/80 for scaler...\n",
      "  ...processing patient 43/80 for scaler...\n",
      "  ...processing patient 44/80 for scaler...\n",
      "  ...processing patient 45/80 for scaler...\n",
      "  ...processing patient 46/80 for scaler...\n",
      "  ...processing patient 47/80 for scaler...\n",
      "  ...processing patient 48/80 for scaler...\n",
      "  ...processing patient 49/80 for scaler...\n",
      "  ...processing patient 50/80 for scaler...\n",
      "  ...processing patient 51/80 for scaler...\n",
      "  ...processing patient 52/80 for scaler...\n",
      "  ...processing patient 53/80 for scaler...\n",
      "  ...processing patient 54/80 for scaler...\n",
      "  ...processing patient 55/80 for scaler...\n",
      "  ...processing patient 56/80 for scaler...\n",
      "  ...processing patient 57/80 for scaler...\n",
      "  ...processing patient 58/80 for scaler...\n",
      "  ...processing patient 59/80 for scaler...\n",
      "  ...processing patient 60/80 for scaler...\n",
      "  ...processing patient 61/80 for scaler...\n",
      "  ...processing patient 62/80 for scaler...\n",
      "  ...processing patient 63/80 for scaler...\n",
      "  ...processing patient 64/80 for scaler...\n",
      "  ...processing patient 65/80 for scaler...\n",
      "  ...processing patient 66/80 for scaler...\n",
      "  ...processing patient 67/80 for scaler...\n",
      "  ...processing patient 68/80 for scaler...\n",
      "  ...processing patient 69/80 for scaler...\n",
      "  ...processing patient 70/80 for scaler...\n",
      "  ...processing patient 71/80 for scaler...\n",
      "  ...processing patient 72/80 for scaler...\n",
      "  ...processing patient 73/80 for scaler...\n",
      "  ...processing patient 74/80 for scaler...\n",
      "  ...processing patient 75/80 for scaler...\n",
      "  ...processing patient 76/80 for scaler...\n",
      "  ...processing patient 77/80 for scaler...\n",
      "  ...processing patient 78/80 for scaler...\n",
      "  ...processing patient 79/80 for scaler...\n",
      "  ...processing patient 80/80 for scaler...\n",
      "--- Scaler fitting complete. Saving to standard_scaler_100.joblib ---\n",
      "--- Data preparation and scaling complete. ---\n"
     ]
    }
   ],
   "source": [
    "# --- Download Data ---\n",
    "os.makedirs(VITAL_FILES_LOCAL_DIR, exist_ok=True)\n",
    "s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "S3_BUCKET_NAME, S3_BASE_KEY = 'physionet-open', 'vitaldb/1.0.0/vital_files/'\n",
    "print(f\"Downloading {NUM_PATIENTS_TO_PROCESS} files...\")\n",
    "for i in range(1, NUM_PATIENTS_TO_PROCESS + 1):\n",
    "    file_name = f'{i:04d}.vital'\n",
    "    local_file_path = os.path.join(VITAL_FILES_LOCAL_DIR, file_name)\n",
    "    if not os.path.exists(local_file_path):\n",
    "        try: s3_client.download_file(S3_BUCKET_NAME, os.path.join(S3_BASE_KEY, file_name), local_file_path)\n",
    "        except Exception as e: print(f\"  ERROR downloading {file_name}: {e}\")\n",
    "print(\"--- Download complete. ---\\n\")\n",
    "\n",
    "# --- Prepare Patient ID Split ---\n",
    "all_case_ids = list(range(1, NUM_PATIENTS_TO_PROCESS + 1))\n",
    "train_case_ids, val_case_ids = train_test_split(all_case_ids, test_size=(1 - TRAIN_TEST_SPLIT_RATIO), random_state=RANDOM_SEED)\n",
    "print(f\"Patient Split: {len(train_case_ids)} for Training, {len(val_case_ids)} for Validation.\")\n",
    "\n",
    "# --- Fit or Load the StandardScaler ---\n",
    "if os.path.exists(SCALER_SAVE_PATH):\n",
    "    print(f\"\\n--- Loading existing scaler from {SCALER_SAVE_PATH} ---\")\n",
    "    scaler = joblib.load(SCALER_SAVE_PATH)\n",
    "else:\n",
    "    print(\"\\n--- Fitting new StandardScaler on Training Data ---\")\n",
    "    scaler = StandardScaler()\n",
    "    for i, case_id in enumerate(train_case_ids):\n",
    "        print(f\"  ...processing patient {i+1}/{len(train_case_ids)} for scaler...\")\n",
    "        file_path = os.path.join(VITAL_FILES_LOCAL_DIR, f'{case_id:04d}.vital')\n",
    "        if not os.path.exists(file_path): continue\n",
    "        vf = vitaldb.VitalFile(file_path)\n",
    "        df = vf.to_pandas(INPUT_VITALS, interval=1)\n",
    "        df.ffill(inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        if not df.empty:\n",
    "            scaler.partial_fit(df[INPUT_VITALS])\n",
    "    print(f\"--- Scaler fitting complete. Saving to {SCALER_SAVE_PATH} ---\")\n",
    "    joblib.dump(scaler, SCALER_SAVE_PATH)\n",
    "\n",
    "print(\"--- Data preparation and scaling complete. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704f7347",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T04:27:20.251044Z",
     "iopub.status.busy": "2025-07-11T04:27:20.250337Z",
     "iopub.status.idle": "2025-07-11T14:49:42.334305Z",
     "shell.execute_reply": "2025-07-11T14:49:42.333600Z"
    },
    "papermill": {
     "duration": 37345.45082,
     "end_time": "2025-07-11T14:49:45.695121",
     "exception": false,
     "start_time": "2025-07-11T04:27:20.244301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing Training Data Generator (80 patients) ---\n",
      "--- Training Generator ready. Total sequences: 695274 ---\n",
      "\n",
      "--- Preparing Validation Data Generator (20 patients) ---\n",
      "--- Validation Generator ready. Total sequences: 159529 ---\n",
      "\n",
      "--- Building model with increased capacity: embed_dim=128, ff_dim=256, num_heads=8 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1752208322.972449      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1752208322.973066      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ positional_encoding             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">38,400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncoding</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">593,920</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">593,920</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,632</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ positional_encoding             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m38,400\u001b[0m │\n",
       "│ (\u001b[38;5;33mPositionalEncoding\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m593,920\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m593,920\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m9,632\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,236,545</span> (4.72 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,236,545\u001b[0m (4.72 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,236,545</span> (4.72 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,236,545\u001b[0m (4.72 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training for 25 epoch(s) ---\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752208335.835287     495 service.cc:148] XLA service 0x7e8f2000d4c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1752208335.836712     495 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1752208335.836731     495 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1752208337.336461     495 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1752208349.712315     495 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1487s\u001b[0m 539ms/step - loss: 1270.5769 - mean_absolute_error: 30.5969 - val_loss: 210.4020 - val_mean_absolute_error: 9.0794\n",
      "Epoch 2/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1474s\u001b[0m 543ms/step - loss: 236.4269 - mean_absolute_error: 10.1908 - val_loss: 195.2508 - val_mean_absolute_error: 8.8207\n",
      "Epoch 3/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1474s\u001b[0m 543ms/step - loss: 246.4339 - mean_absolute_error: 10.2140 - val_loss: 193.0101 - val_mean_absolute_error: 8.7294\n",
      "Epoch 4/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1477s\u001b[0m 544ms/step - loss: 227.3939 - mean_absolute_error: 9.8586 - val_loss: 207.1676 - val_mean_absolute_error: 9.5778\n",
      "Epoch 5/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1477s\u001b[0m 544ms/step - loss: 234.2221 - mean_absolute_error: 9.8423 - val_loss: 197.4386 - val_mean_absolute_error: 9.0717\n",
      "Epoch 6/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1477s\u001b[0m 544ms/step - loss: 234.4183 - mean_absolute_error: 9.8519 - val_loss: 278.0918 - val_mean_absolute_error: 9.6125\n",
      "Epoch 7/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1478s\u001b[0m 544ms/step - loss: 223.2399 - mean_absolute_error: 9.6183 - val_loss: 196.0845 - val_mean_absolute_error: 8.8492\n",
      "Epoch 8/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1480s\u001b[0m 545ms/step - loss: 212.0222 - mean_absolute_error: 9.4821 - val_loss: 202.1733 - val_mean_absolute_error: 9.0494\n",
      "Epoch 9/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1482s\u001b[0m 546ms/step - loss: 225.9141 - mean_absolute_error: 9.7161 - val_loss: 192.5556 - val_mean_absolute_error: 8.9802\n",
      "Epoch 10/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1480s\u001b[0m 545ms/step - loss: 226.5095 - mean_absolute_error: 9.6741 - val_loss: 192.5987 - val_mean_absolute_error: 8.6144\n",
      "Epoch 11/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1481s\u001b[0m 545ms/step - loss: 224.2978 - mean_absolute_error: 9.6526 - val_loss: 240.1939 - val_mean_absolute_error: 9.1831\n",
      "Epoch 12/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1481s\u001b[0m 545ms/step - loss: 203.2895 - mean_absolute_error: 9.1827 - val_loss: 183.4967 - val_mean_absolute_error: 8.3452\n",
      "Epoch 13/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1485s\u001b[0m 547ms/step - loss: 223.7977 - mean_absolute_error: 9.6171 - val_loss: 239.9417 - val_mean_absolute_error: 9.0765\n",
      "Epoch 14/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1485s\u001b[0m 547ms/step - loss: 220.0462 - mean_absolute_error: 9.5570 - val_loss: 198.0728 - val_mean_absolute_error: 8.9631\n",
      "Epoch 15/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1483s\u001b[0m 546ms/step - loss: 219.9280 - mean_absolute_error: 9.6096 - val_loss: 191.5279 - val_mean_absolute_error: 8.6422\n",
      "Epoch 16/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1484s\u001b[0m 547ms/step - loss: 229.3595 - mean_absolute_error: 9.7057 - val_loss: 229.4069 - val_mean_absolute_error: 9.1135\n",
      "Epoch 17/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1484s\u001b[0m 546ms/step - loss: 212.1000 - mean_absolute_error: 9.2869 - val_loss: 263.2938 - val_mean_absolute_error: 9.3742\n",
      "Epoch 18/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1484s\u001b[0m 546ms/step - loss: 223.6761 - mean_absolute_error: 9.3953 - val_loss: 190.9570 - val_mean_absolute_error: 8.6739\n",
      "Epoch 19/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1485s\u001b[0m 547ms/step - loss: 199.6103 - mean_absolute_error: 9.1656 - val_loss: 234.0553 - val_mean_absolute_error: 9.1286\n",
      "Epoch 20/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1484s\u001b[0m 546ms/step - loss: 205.8134 - mean_absolute_error: 9.0449 - val_loss: 195.1496 - val_mean_absolute_error: 8.8074\n",
      "Epoch 21/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1486s\u001b[0m 547ms/step - loss: 224.0193 - mean_absolute_error: 9.5260 - val_loss: 195.9057 - val_mean_absolute_error: 8.8018\n",
      "Epoch 22/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1485s\u001b[0m 547ms/step - loss: 191.0816 - mean_absolute_error: 8.8577 - val_loss: 186.6911 - val_mean_absolute_error: 8.6559\n",
      "Epoch 23/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1487s\u001b[0m 548ms/step - loss: 210.8622 - mean_absolute_error: 9.2087 - val_loss: 187.6527 - val_mean_absolute_error: 8.6646\n",
      "Epoch 24/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1488s\u001b[0m 548ms/step - loss: 189.8338 - mean_absolute_error: 8.8437 - val_loss: 187.5719 - val_mean_absolute_error: 8.5166\n",
      "Epoch 25/25\n",
      "\u001b[1m2716/2716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1488s\u001b[0m 548ms/step - loss: 212.0642 - mean_absolute_error: 9.2035 - val_loss: 190.3337 - val_mean_absolute_error: 8.6723\n",
      "--- Training complete. ---\n",
      "\n",
      "\u001b[92mSUCCESS: Smoke test PASSED. Final training loss is 204.3010.\u001b[0m\n",
      "The model is numerically stable. It is now safe to increase patient count and epochs.\n"
     ]
    }
   ],
   "source": [
    "# --- Create Data Generators ---\n",
    "# This step is now much faster as the heavy lifting (scaling) is done.\n",
    "train_generator = MemorySafeVitalDBGenerator(\"Training\", train_case_ids, VITAL_FILES_LOCAL_DIR, BATCH_SIZE, SEQUENCE_LENGTH, INPUT_VITALS, TARGET_VITAL, scaler=scaler)\n",
    "val_generator = MemorySafeVitalDBGenerator(\"Validation\", val_case_ids, VITAL_FILES_LOCAL_DIR, BATCH_SIZE, SEQUENCE_LENGTH, INPUT_VITALS, TARGET_VITAL, scaler=scaler)\n",
    "\n",
    "# --- Check if generators are valid ---\n",
    "if len(train_generator.index_map) == 0:\n",
    "    print(\"FATAL: Training generator is empty. Cannot proceed.\")\n",
    "else:\n",
    "    # Handle potentially empty validation generator\n",
    "    validation_data_for_fit = val_generator if len(val_generator.index_map) > 0 else None\n",
    "    if validation_data_for_fit is None:\n",
    "        print(\"\\nWARNING: Validation generator is empty. Proceeding with training but without validation.\")\n",
    "\n",
    "    # --- Build and Compile Model ---\n",
    "    transformer_model = build_transformer_model(SEQUENCE_LENGTH, len(INPUT_VITALS))\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE, clipnorm=CLIPNORM)\n",
    "    transformer_model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "    transformer_model.summary()\n",
    "\n",
    "    # --- Train for 1 Epoch ---\n",
    "    print(f\"\\n--- Starting Training for {EPOCHS} epoch(s) ---\")\n",
    "    history = transformer_model.fit(\n",
    "        train_generator, \n",
    "        epochs=EPOCHS, \n",
    "        validation_data=validation_data_for_fit, # Use the potentially None variable\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"--- Training complete. ---\")\n",
    "\n",
    "    # --- Final Verification ---\n",
    "    final_loss = history.history['loss'][-1]\n",
    "    if np.isnan(final_loss):\n",
    "        print(\"\\n\\033[91mERROR: Smoke test FAILED. Loss is NaN.\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\n\\033[92mSUCCESS: Smoke test PASSED. Final training loss is {final_loss:.4f}.\\033[0m\")\n",
    "        print(\"The model is numerically stable. It is now safe to increase patient count and epochs.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 37645.639642,
   "end_time": "2025-07-11T14:49:51.746590",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-11T04:22:26.106948",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
